{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "struct Constant{T} <: GraphNode\n",
    "    output :: T\n",
    "end\n",
    "\n",
    "mutable struct Variable <: GraphNode\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    Variable(output; name=\"?\") = new(output, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct ScalarOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    ScalarOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct BroadcastedOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    BroadcastedOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "\n",
    "import Base: show, summary\n",
    "show(io::IO, x::ScalarOperator{F}) where {F} = print(io, \"op \", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::BroadcastedOperator{F}) where {F} = print(io, \"op.\", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::Constant) = print(io, \"const \", x.output)\n",
    "show(io::IO, x::Variable) = begin\n",
    "    print(io, \"var \", x.name);\n",
    "    print(io, \"\\n ┣━ ^ \"); summary(io, x.output)\n",
    "    print(io, \"\\n ┗━ ∇ \");  summary(io, x.gradient)\n",
    "end\n",
    "\n",
    "function visit(node::GraphNode, visited, order) \n",
    "    if node ∉ visited\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end \n",
    "end\n",
    "\n",
    "function visit(node::Operator, visited, order) \n",
    "    if node ∉ visited\n",
    "        push!(visited, node)\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(order, node)\n",
    "    end \n",
    "end\n",
    "\n",
    "function topological_sort(head::GraphNode) \n",
    "    visited = Set()\n",
    "    order = Vector() \n",
    "    visit(head, visited, order) \n",
    "    return order\n",
    "end\n",
    "\n",
    "reset!(node::Constant) = nothing\n",
    "reset!(node::Variable) = node.gradient = nothing\n",
    "reset!(node::Operator) = node.gradient = nothing\n",
    "\n",
    "compute!(node::Constant) = nothing\n",
    "compute!(node::Variable) = nothing\n",
    "compute!(node::Operator) = node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "\n",
    "function forward!(order::Vector)\n",
    "    for node in order\n",
    "        compute!(node)\n",
    "        reset!(node)\n",
    "    end\n",
    "    return last(order).output\n",
    "end\n",
    "\n",
    "update!(node::Constant, gradient) = nothing\n",
    "update!(node::GraphNode, gradient) = if isnothing(node.gradient)\n",
    "    node.gradient = gradient else node.gradient .+= gradient\n",
    "end\n",
    "\n",
    "function backward!(order::Vector; seed=1.0)\n",
    "    result = last(order)\n",
    "    result.gradient = seed\n",
    "    @assert length(result.output) == 1\n",
    "    for node in reverse(order)\n",
    "        backward!(node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function backward!(node::Constant) end\n",
    "function backward!(node::Variable) end\n",
    "function backward!(node::Operator)\n",
    "    inputs = node.inputs\n",
    "    gradients = backward(node, [input.output for input in inputs]..., node.gradient)\n",
    "    for (input, gradient) in zip(inputs, gradients)\n",
    "        update!(input, gradient)\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 11 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import LinearAlgebra: diagm\n",
    "import LinearAlgebra: mul!\n",
    "\n",
    "import Base: *\n",
    "*(A::GraphNode, x::GraphNode) = BroadcastedOperator(mul!, A, x)\n",
    "forward(::BroadcastedOperator{typeof(mul!)}, A, x) = return A * x\n",
    "backward(::BroadcastedOperator{typeof(mul!)}, A, x, g) = tuple(g * x', A' * g)\n",
    "\n",
    "Base.Broadcast.broadcasted(*, x::GraphNode, y::GraphNode) = BroadcastedOperator(*, x, y)\n",
    "forward(::BroadcastedOperator{typeof(*)}, x, y) = return x .* y\n",
    "backward(node::BroadcastedOperator{typeof(*)}, x, y, g) = let\n",
    "    o = ones(length(node.output))\n",
    "    Jx = diagm(vec(y .* o))\n",
    "    Jy = diagm(vec(x .* o))\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "\n",
    "import Base: exp\n",
    "Base.Broadcast.broadcasted(exp, x::GraphNode) = BroadcastedOperator(exp, x)\n",
    "forward(::BroadcastedOperator{typeof(exp)}, x) = return exp.(x)\n",
    "backward(node::BroadcastedOperator{typeof(exp)}, x, grad) = let\n",
    "    o = ones(length(node.output))\n",
    "    J = diagm(vec(node.output .* o))\n",
    "    tuple(J' * grad)\n",
    "end\n",
    "\n",
    "import Base: sum\n",
    "sum(x::GraphNode) = BroadcastedOperator(sum, x)\n",
    "forward(::BroadcastedOperator{typeof(sum)}, x) = return sum(x)\n",
    "backward(::BroadcastedOperator{typeof(sum)}, x, g) = let\n",
    "    o = ones(length(x))\n",
    "    J = o'\n",
    "    tuple(J' * g)\n",
    "end\n",
    "\n",
    "Base.Broadcast.broadcasted(/, x::GraphNode, y::GraphNode) = BroadcastedOperator(/, x, y)\n",
    "forward(::BroadcastedOperator{typeof(/)}, x, y) = return x ./ y\n",
    "backward(node::BroadcastedOperator{typeof(/)}, x, y::Real, g) = let\n",
    "    o = ones(length(node.output))\n",
    "    Jx = diagm(vec(o ./ y))\n",
    "    Jy = (-x ./ y .^2)\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "\n",
    "import Base: log\n",
    "Base.Broadcast.broadcasted(log, x::GraphNode) = BroadcastedOperator(log, x)\n",
    "forward(::BroadcastedOperator{typeof(log)}, x) = return log.(x)\n",
    "backward(::BroadcastedOperator{typeof(log)}, x, g) = let\n",
    "    o = ones(length(x))\n",
    "    J = o' ./ x\n",
    "    tuple(J' * g)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dense (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function convOperation(I, K)\n",
    "    n, m = size(I) .- size(K) .+ 1\n",
    "    J = zeros(n, m)\n",
    "    for i=1:n, j=1:m\n",
    "        J[i, j] = sum(I[i:i+1, j:j+1] .* K)\n",
    "    end\n",
    "    return J\n",
    "end\n",
    "convLayer(x::GraphNode, k::GraphNode) = BroadcastedOperator(convLayer, x, k)\n",
    "forward(::BroadcastedOperator{typeof(convLayer)}, x, k) = let\n",
    "    return convOperation(x, k)\n",
    "end\n",
    "backward(node::BroadcastedOperator{typeof(convLayer)}, I, K, g) = let\n",
    "    kh, kw = size(K)\n",
    "    fgrad = zeros(Float32, size(K))\n",
    "    outh, outw = size(node.output)\n",
    "\n",
    "    for i in 1:outh\n",
    "        for j in 1:outw\n",
    "            fgrad += g[i, j] * I[i:i+kh-1, j:j+kw-1]\n",
    "        end    \n",
    "    end\n",
    "    return fgrad\n",
    "end\n",
    "function convlayerInit(x, k , activation) return activation(convLayer(x, k)) end\n",
    "function convlayerInit(x, k) return convLayer(x, k) end\n",
    "\n",
    "function flatten(input)       \n",
    "    return reshape(input, (:, 1))\n",
    "end\n",
    "flatten(x::GraphNode) = BroadcastedOperator(flatten, x)\n",
    "forward(::BroadcastedOperator{typeof(flatten)}, x) = return flatten(x)\n",
    "backward(::BroadcastedOperator{typeof(flatten)}, x, grad) = let\n",
    "    result = reshape(grad, size(x))\n",
    "    tuple(result)\n",
    "end\n",
    "\n",
    "function dense(w, x, activation) return activation(w * x) end\n",
    "function dense(w, x) return w * x end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "load_fashion_mnist (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using MLDatasets\n",
    "using Flux: onehotbatch\n",
    "function load_fashion_mnist()\n",
    "\n",
    "    train_data, train_labels = FashionMNIST(split=:train)[1:10]\n",
    "    test_data, test_labels = FashionMNIST(split=:test)[1:10]\n",
    "\n",
    "    train_data = train_data ./ 255.0\n",
    "    test_data = test_data ./ 255.0\n",
    "\n",
    "    train_data = reshape(train_data, (:, 1, 28, 28))\n",
    "    test_data = reshape(test_data, (:, 1, 28, 28))\n",
    "\n",
    "    train_labels = onehotbatch(train_labels, 0:9)\n",
    "    test_labels = onehotbatch(test_labels, 0:9)\n",
    "\n",
    "    X = Variable(train_data, name=\"images\")\n",
    "    y = Variable(train_labels, name = \"labels\")\n",
    "\n",
    "    return X, y, test_data, test_labels\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainSGD (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "relu(x::GraphNode) = BroadcastedOperator(relu, x)\n",
    "forward(::BroadcastedOperator{typeof(relu)}, x) = return max.(0, x)\n",
    "backward(::BroadcastedOperator{typeof(relu)}, x, g) = let \n",
    "        result = g .* (x .>= 0)\n",
    "        return tuple(result)\n",
    " end\n",
    "\n",
    "elu(x::GraphNode) = BroadcastedOperator(elu, x)\n",
    "forward(::BroadcastedOperator{typeof(elu)}, x) = return (x .>= 0) .* x + 1 .* (exp.(x) .- 1) .* (x .< 0)\n",
    "backward(::BroadcastedOperator{typeof(elu)}, x, g) = let\n",
    "        grad = g .* ((x .>= 0) .+ 1 .* exp.(x) .* (x .< 0))\n",
    "        return tuple(grad)\n",
    "end\n",
    "\n",
    "softmax(x::GraphNode) = BroadcastedOperator(softmax, x)\n",
    "forward(::BroadcastedOperator{typeof(softmax)}, x) = exp.(x) ./ sum(exp.(x))\n",
    "backward(node::BroadcastedOperator{typeof(softmax)}, x, g) = let\n",
    "        y = ones(length(node.output))\n",
    "        J = diagm(vec(node.output .* y)) - node.output * node.output'\n",
    "        tuple(J' * g)\n",
    "end\n",
    "\n",
    "function cross_entropy_loss(x::GraphNode, y::GraphNode)\n",
    "        return sum(y .* log.(exp.(x))) * Constant(-1.0)\n",
    "end\n",
    "\n",
    "function net(x, wc, wd, wo, y)\n",
    "        c = convlayerInit(x, wc, relu)\n",
    "        c.name = \"conv layer\"\n",
    "        f = flatten(c)\n",
    "        d1 = dense(wd, f, elu)\n",
    "        d1.name = \"dense leyer\"\n",
    "        d2 = dense(wo, d1, relu)\n",
    "        d2.name = \"output\"\n",
    "        E = cross_entropy_loss(y, d2)\n",
    "        E.name = \"loss\"\n",
    "    \n",
    "        return topological_sort(E)\n",
    "end\n",
    "\n",
    "function inicializeTestData()\n",
    "        Wc  = Variable(rand(2,2), name=\"Wagi conv\")\n",
    "        Wd  = Variable(rand(10,729), name=\"Wagi dense\")\n",
    "        Wo  = Variable(rand(10,10), name=\"Wagi out\")\n",
    "        X, Y, test_data, test_labels = load_fashion_mnist()  \n",
    "        return Wc,Wd,Wo,X, Y\n",
    "end\n",
    "\n",
    "function trainSGD(epochs::Int, learning_rate::Real, expectedValueOfLoss::Real)\n",
    "\n",
    "        Wc, Wd, Wo, X, Y = inicializeTestData()\n",
    "        forwardVal = 0.0\n",
    "        for epoch in 1:epochs\n",
    "                for j in 1:size(X.output, 1)\n",
    "                        x = Variable(X.output[j, 1, :, :])\n",
    "                        y = Variable(Y.output[:, j])\n",
    "                        graph = net(x, Wc, Wd, Wo, y)\n",
    "                        forwardVal = forward!(graph)\n",
    "                        if abs(forwardVal) < expectedValueOfLoss\n",
    "                                print(\"Epoch nr. \", epoch, \" Loss: \", round(forwardVal, digits=6), \"\\n\")\n",
    "                                return\n",
    "                        end\n",
    "                        backward!(graph)\n",
    "                        if (forwardVal>0)\n",
    "                        Wc.output .-= learning_rate .* Wc.gradient\n",
    "                        Wd.output .-= learning_rate .* Wd.gradient\n",
    "                        Wo.output .-= learning_rate .* Wo.gradient\n",
    "                        else\n",
    "                                Wc.output .+= learning_rate .* Wc.gradient\n",
    "                                Wd.output .+= learning_rate .* Wd.gradient\n",
    "                                Wo.output .+= learning_rate .* Wo.gradient\n",
    "                        end\n",
    "                end\n",
    "                print(\"Epoch nr. \", epoch, \" Loss: \", round(forwardVal, digits=6), \"\\n\")\n",
    "        end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch nr. 1 Loss: -4.234983\n",
      "Epoch nr. 2 Loss: -4.061186\n",
      "Epoch nr. 3 Loss: -3.889983\n",
      "Epoch nr. 4 Loss: -3.721267\n",
      "Epoch nr. 5 Loss: -3.55493\n",
      "Epoch nr. 6 Loss: -3.390867\n",
      "Epoch nr. 7 Loss: -3.228973\n",
      "Epoch nr. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Loss: -3.069147\n",
      "Epoch nr. 9 Loss: -2.911285\n",
      "Epoch nr. 10 Loss: -2.755289\n",
      "Epoch nr. 11 Loss: -2.601059\n",
      "Epoch nr. 12 Loss: -2.448497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch nr. 13 Loss: -2.297507\n",
      "Epoch nr. 14 Loss: -2.147993\n",
      "Epoch nr. 15 Loss: -1.999859\n",
      "Epoch nr. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 Loss: -1.856836\n",
      "Epoch nr. 17 Loss: -1.716372\n",
      "Epoch nr. 18 Loss: -1.577984\n",
      "Epoch nr. 19 Loss: -1.444295\n",
      "Epoch nr. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Loss: -1.31673\n",
      "Epoch nr. 21 Loss: -1.191693\n",
      "Epoch nr. 22 Loss: -1.069817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch nr. 23 Loss: -0.954566\n",
      "Epoch nr. 24 Loss: -0.845227\n",
      "Epoch nr. 25 Loss: -0.739913\n",
      "Epoch nr. 26 Loss: -0.637605\n",
      "Epoch nr. 27 Loss: -0.541069\n",
      "Epoch nr. 28 Loss: -0.458316\n",
      "Epoch nr. 29 Loss: -0.386488\n",
      "Epoch nr. 30 Loss: -0.328714\n",
      "Epoch nr. 31 Loss: -0.286841\n",
      "Epoch nr. 32 Loss: -0.2509\n",
      "Epoch nr. 33 Loss: -0.221252\n",
      "Epoch nr. 34 Loss: -0.199701\n",
      "Epoch nr. 35 Loss: -0.184162\n",
      "Epoch nr. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 Loss: -0.170782\n",
      "Epoch nr. 37 Loss: -0.158488\n",
      "Epoch nr. 38 Loss: -0.147134\n",
      "Epoch nr. 39 Loss: -0.137323\n",
      "Epoch nr. 40 Loss: -0.128374\n",
      "Epoch nr. 41 Loss: -0.119932\n",
      "Epoch nr. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 Loss: -0.112068\n",
      "Epoch nr. 43 Loss: -0.104635\n",
      "Epoch nr. 44 Loss: -0.097573\n",
      "Epoch nr. 45 Loss: -0.090743\n",
      "Epoch nr. 46 Loss: -0.084129\n",
      "Epoch nr. 47 Loss: -0.077728\n",
      "Epoch nr. 48 Loss: -0.071809\n",
      "Epoch nr. 49 Loss: -0.066119\n",
      "Epoch nr. 50 Loss: -0.060642\n",
      "Epoch nr. 51 Loss: -0.055476\n",
      "Epoch nr. 52 Loss: -0.050769\n",
      "Epoch nr. 53 Loss: -0.046445\n",
      "Epoch nr. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54 Loss: -0.042537\n",
      "Epoch nr. 55 Loss: -0.039074\n",
      "Epoch nr. 56 Loss: -0.036174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch nr. 57 Loss: -0.033844\n",
      "Epoch nr. 58 Loss: -0.032283\n",
      "Epoch nr. 59 Loss: -0.031155\n",
      "Epoch nr. 60 Loss: -0.03026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch nr. 61 Loss: -0.029447\n",
      "Epoch nr. 62 Loss: -0.028672\n",
      "Epoch nr. 63 Loss: -0.027917\n",
      "Epoch nr. 64 Loss: -0.027205\n",
      "Epoch nr. 65 Loss: -0.026506\n",
      "Epoch nr. 66 Loss: -0.025869\n",
      "Epoch nr. 67 Loss: -0.025241\n",
      "Epoch nr. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 Loss: -0.02462\n",
      "Epoch nr. 69 Loss: -0.024029\n",
      "Epoch nr. 70 Loss: -0.023469\n",
      "Epoch nr. 71 Loss: -0.022916\n",
      "Epoch nr. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 Loss: -0.022367\n",
      "Epoch nr. 73 Loss: -0.021823\n",
      "Epoch nr. 74 Loss: -0.021284\n",
      "Epoch nr. 75 Loss: -0.020745\n",
      "Epoch nr. 76 Loss: -0.020213\n",
      "Epoch nr. 77 Loss: -0.019683\n",
      "Epoch nr. "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 Loss: -0.019155\n",
      "Epoch nr. 79 Loss: -0.018631\n",
      "Epoch nr. 80 Loss: -0.018107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch nr. 81 Loss: -0.017587\n",
      "Epoch nr. 82 Loss: -0.017069\n",
      "Epoch nr. 83 Loss: -0.016558\n",
      "Epoch nr. 84 Loss: -0.016054\n",
      "Epoch nr. 85 Loss: -0.015552\n",
      "Epoch nr. 86 Loss: -0.01505\n",
      "Epoch nr. 87 Loss: -0.014549\n",
      "Epoch nr. 88 Loss: -0.014049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch nr. 89 Loss: -0.013549\n",
      "Epoch nr. 90 Loss: -0.013051\n",
      "Epoch nr. 91 Loss: -0.012569\n",
      "Epoch nr. 92 Loss: -0.012094\n",
      "Epoch nr. 93 Loss: -0.011623\n",
      "Epoch nr. 94 Loss: -0.011154\n",
      "Epoch nr. 95 Loss: -0.010685\n",
      "Epoch nr. 96 Loss: -0.010219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch nr. 97 Loss: -0.00977\n",
      "Epoch nr. 98 Loss: -0.009336\n",
      "Epoch nr. 99 Loss: -0.008903\n",
      "Epoch nr. 100 Loss: -0.008476\n"
     ]
    }
   ],
   "source": [
    "trainSGD(100, 0.001, 0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
